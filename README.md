# Book-price-prediction-model
Fine-tune (Llama-3.1-8B) Book price prediction based on description 


Qu√° tr√¨nh Fine-tuning cho d·ª± √°n n√†y
1. Dataset
Ch√∫ng ta s·ª≠ d·ª•ng b·ªô d·ªØ li·ªáu t·ª´ Hugging Face: üëâ Amazon-Reviews-2023-Books-Meta

2. Data Preparation Process
Quy tr√¨nh x·ª≠ l√Ω d·ªØ li·ªáu g·ªìm 4 b∆∞·ªõc:

Data Cleaning
Lo·∫°i b·ªè c√°c th√¥ng tin kh√¥ng li√™n quan nh∆∞ m√£ s·∫£n ph·∫©m, k√Ω t·ª± ƒë·∫∑c bi·ªát v√† c√°c tr∆∞·ªùng th√¥ng tin kh√¥ng gi√∫p √≠ch cho vi·ªác d·ª± ƒëo√°n gi√°.

Tokenization
S·ª≠ d·ª•ng tokenizer c·ªßa m√¥ h√¨nh Llama ƒë·ªÉ ƒë·∫øm v√† gi·ªõi h·∫°n s·ªë l∆∞·ª£ng token trong m·ªói v√≠ d·ª•.

Quality Filtering
Ch·ªâ gi·ªØ l·∫°i nh·ªØng m·∫´u d·ªØ li·ªáu:

ƒê·ªß n·ªôi dung h·ªØu √≠ch (h∆°n 150 tokens).

Kh√¥ng qu√° d√†i (d∆∞·ªõi 160 tokens).

Prompt Standardization
Chu·∫©n h√≥a t·∫•t c·∫£ c√°c prompt theo c·∫•u tr√∫c:

   **Question: "How much does this cost to the nearest dollar?"**
   **Content: (n·ªôi dung s√°ch)**
   **Answer: (gi√°)**
3. Machine Learning Modeling (Before Fine-tuning)
Random model: Sai s·ªë: $423.71 (r·∫•t k√©m).

Linear Regression: Sai s·ªë gi·∫£m c√≤n $52.94.

Word2Vec: Sai s·ªë c√≤n $50.94.

HistGradientBoosting: Sai s·ªë th·∫•p nh·∫•t $50.74.

4. Fine-tuning Techniques
LoRA (Low-Rank Adaptation): T√†i li·ªáu: LoRA Guide - Huggingface

QLoRA (Quantized LoRA): T√†i li·ªáu: QLoRA Guide - Huggingface

Paper tham kh·∫£o: üìÑ QLoRA Paper (2023)

5. Important Hyperparameters
5.1 R (Rank)
√ù nghƒ©a: X√°c ƒë·ªãnh k√≠ch th∆∞·ªõc kh√¥ng gian con trong LoRA (s·ªë l∆∞·ª£ng tham s·ªë th·∫•p h∆°n ƒë∆∞·ª£c hu·∫•n luy·ªán).

Gi√° tr·ªã c√†ng nh·ªè ‚Üí √≠t tham s·ªë c·∫ßn c·∫≠p nh·∫≠t ‚Üí nhanh h∆°n, nh∆∞ng c√≥ th·ªÉ gi·∫£m hi·ªáu qu·∫£ fine-tuning.

Docs: PEFT LoRA Parameters

5.2 Alpha
√ù nghƒ©a: H·ªá s·ªë scale khu·∫øch ƒë·∫°i ·∫£nh h∆∞·ªüng c·ªßa c√°c tham s·ªë LoRA l√™n tr·ªçng s·ªë ban ƒë·∫ßu.

Alpha c√†ng l·ªõn ‚Üí ·∫£nh h∆∞·ªüng LoRA c√†ng m·∫°nh m·∫Ω.

Docs: LoRA Alpha Scaling (g·ªëc LoRA paper).

5.3 Target Modules
√ù nghƒ©a: C√°c module trong m√¥ h√¨nh m√† LoRA s·∫Ω √°p d·ª•ng.

V√≠ d·ª•: "q_proj", "k_proj", "v_proj", "o_proj" (c√°c l·ªõp attention projection).

Docs: Choosing Target Modules

6. LLMs Training Process
6.1 Forward Pass
ƒê∆∞a d·ªØ li·ªáu ƒë·∫ßu v√†o qua m√¥ h√¨nh ƒë·ªÉ m√¥ h√¨nh d·ª± ƒëo√°n token ti·∫øp theo.

6.2 Loss Calculation
So s√°nh ƒë·∫ßu ra c·ªßa m√¥ h√¨nh v·ªõi nh√£n th·ª±c t·∫ø ƒë·ªÉ t√≠nh to√°n loss.

6.3 Backward Pass
T√≠nh to√°n gradient c·ªßa loss v·ªõi respect t·ªõi c√°c tham s·ªë m√¥ h√¨nh.

6.4 Optimization
C·∫≠p nh·∫≠t tr·ªçng s·ªë theo c√¥ng th·ª©c:

new_weight = old_weight - learning_rate * gradient

üìö Docs: Deep Learning Book - Optimization

7. Training Configuration
7.1 SFTTrainer Configuration
Epochs: S·ªë l·∫ßn duy·ªát to√†n b·ªô t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán (c√≥ th·ªÉ tƒÉng l√™n 4+).

Batch size: 1 (per device) √ó 16 (gradient accumulation) = 16 m·∫´u/l·∫ßn update.

7.2 Optimizer
paged_adamw_32bit:

Phi√™n b·∫£n AdamW t·ªëi ∆∞u b·ªô nh·ªõ khi fine-tune m√¥ h√¨nh l·ªõn.

T√†i li·ªáu: PagedAdamW Optimizer

7.3 Learning Rate
Tham s·ªë quy·∫øt ƒë·ªãnh t·ªëc ƒë·ªô c·∫≠p nh·∫≠t tr·ªçng s·ªë.

N·∫øu qu√° cao ‚Üí c√≥ th·ªÉ nh·∫£y kh·ªèi ƒëi·ªÉm t·ªëi ∆∞u.

N·∫øu qu√° th·∫•p ‚Üí m√¥ h√¨nh h·ªçc r·∫•t ch·∫≠m ho·∫∑c k·∫πt local minimum.

7.4 Scheduler
Cosine scheduler:

Learning rate gi·∫£m theo h√¨nh d·∫°ng s√≥ng cosine.

B·∫Øt ƒë·∫ßu gi·∫£m nh·∫π ‚Üí gi·∫£m m·∫°nh ‚Üí ·ªïn ƒë·ªãnh d·∫ßn.

Docs: Cosine LR Scheduler

7.5 Warmup Ratio
Giai ƒëo·∫°n kh·ªüi ƒë·ªông, learning rate tƒÉng d·∫ßn t·ª´ th·∫•p ƒë·∫øn m·ª©c cao nh·∫•t r·ªìi m·ªõi b·∫Øt ƒë·∫ßu gi·∫£m theo scheduler.

M·ªôt s·ªë platforms ph·ªï bi·∫øn tools & framework trong Llms
Hugging Face: N·ªÅn t·∫£ng ph·ªï bi·∫øn cho c√°c m√¥ h√¨nh, b·ªô d·ªØ li·ªáu, b·∫£ng x·∫øp h·∫°ng v√† c·∫£ c√°c ·ª©ng d·ª•ng
LangChain ‚Äì Framework m√£ ngu·ªìn m·ªü cung c·∫•p c√°c k·∫øt n·ªëi nhi·ªÅu thao t√°c v·ªõi M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn (LLM) th√¥ng qua m·ªôt API ƒë∆°n gi·∫£n
Gradio ‚Äì M·ªôt framework UI c·ª±c k·ª≥ ƒë∆°n gi·∫£n, cho ph√©p b·∫°n t·∫°o giao di·ªán ng∆∞·ªùi d√πng prototype ch·ªâ v·ªõi m·ªôt d√≤ng m√£, kh√¥ng c·∫ßn kinh nghi·ªám frontend.
Alternatives ‚Äì C√°c l·ª±a ch·ªçn thay th·∫ø bao g·ªìm Streamlit, Dash v√† Mesop t·ª´ Google
Weights & Biases ‚Äì C√¥ng c·ª• ph√¢n t√≠ch v√† tr·ª±c quan h√≥a trong qu√° tr√¨nh hu·∫•n luy·ªán m√¥ h√¨nh.
Google Colab ‚Äì Vi·∫øt, ƒë√°nh gi√° v√† chia s·∫ª c√°c notebook t·ª´ xa tr√™n m·ªôt m√°y ch·ªß trong Google Cloud.
Modal.com ‚Äì N·ªÅn t·∫£ng AI kh√¥ng m√°y ch·ªß (serverless) d√†nh cho vi·ªác tri·ªÉn khai m√¥ h√¨nh.
**"Ngo√†i ra, n·∫øu b·∫°n mu·ªën h·ªçc s√¢u h∆°n v·ªÅ lƒ©nh v·ª±c n√†y, m√¨nh khuy·∫øn kh√≠ch c√°c b·∫°n tham gia c√°c kh√≥a h·ªçc uy t√≠n ƒë·ªÉ n·∫Øm v·ªØng n·ªÅn t·∫£ng tr∆∞·ªõc khi ƒëi s√¢u v√†o t·ª´ng kh√≠a c·∫°nh."

https://nvdam.widen.net/s/wlbgbqr7cj/nvidia-learning-training-course-catalog

Tai lieu tham khao trong video: Llm course

L∆∞u √Ω: To√†n b·ªô nh·ªØng g√¨ m√¨nh chia s·∫Ω ƒë·ªÅu l√† nh·ªØng g√¨ m√¨nh h·ªçc v√† t·ªïng h·ª£p ƒë∆∞·ª£c. No COMMERCIAL intent!!