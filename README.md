# ğŸ“˜ Book Price Prediction Model  

### ğŸ¯ Fine-tuning Llama 3.1-8B for Predicting Book Prices from Descriptions

---

## ğŸ§  Overview
Dá»± Ã¡n nÃ y táº­p trung vÃ o **fine-tuning mÃ´ hÃ¬nh ngÃ´n ngá»¯ Llama 3.1-8B** Ä‘á»ƒ **dá»± Ä‘oÃ¡n giÃ¡ sÃ¡ch** dá»±a trÃªn mÃ´ táº£ ná»™i dung.  
ToÃ n bá»™ pipeline bao gá»“m cÃ¡c bÆ°á»›c tá»« chuáº©n bá»‹ dá»¯ liá»‡u, huáº¥n luyá»‡n mÃ´ hÃ¬nh cho Ä‘áº¿n Ä‘Ã¡nh giÃ¡ káº¿t quáº£.

---

## ğŸ“Š 1. Dataset
- **Nguá»“n dá»¯ liá»‡u:** [Amazon-Reviews-2023-Books-Meta](https://huggingface.co/datasets) tá»« Hugging Face.  
- Dá»¯ liá»‡u chá»©a thÃ´ng tin mÃ´ táº£ sáº£n pháº©m, tiÃªu Ä‘á», vÃ  giÃ¡ bÃ¡n, phá»¥c vá»¥ cho bÃ i toÃ¡n **price prediction**.

---

## ğŸ§¹ 2. Data Preparation Pipeline

### ğŸ”¸ Step 1: Data Cleaning  
- Loáº¡i bá» cÃ¡c thÃ´ng tin khÃ´ng cáº§n thiáº¿t nhÆ° mÃ£ sáº£n pháº©m, kÃ½ tá»± Ä‘áº·c biá»‡t, hoáº·c cÃ¡c trÆ°á»ng khÃ´ng giÃºp Ã­ch cho viá»‡c dá»± Ä‘oÃ¡n giÃ¡.

### ğŸ”¸ Step 2: Tokenization  
- Sá»­ dá»¥ng tokenizer cá»§a **Llama** Ä‘á»ƒ giá»›i háº¡n sá»‘ lÆ°á»£ng token trong má»—i máº«u.  
- Giá»›i háº¡n:
  - **Tá»‘i thiá»ƒu:** 150 tokens (Ä‘áº£m báº£o Ä‘á»§ ná»™i dung).  
  - **Tá»‘i Ä‘a:** 160 tokens (trÃ¡nh quÃ¡ dÃ i).

### ğŸ”¸ Step 3: Quality Filtering  
- Chá»‰ giá»¯ láº¡i nhá»¯ng máº«u dá»¯ liá»‡u cÃ³ ná»™i dung cháº¥t lÆ°á»£ng vÃ  giÃ¡ há»£p lá»‡.

### ğŸ”¸ Step 4: Prompt Standardization  
Má»i máº«u dá»¯ liá»‡u Ä‘á»u Ä‘Æ°á»£c chuáº©n hÃ³a theo cáº¥u trÃºc:


---

## ğŸ¤– 3. Machine Learning Baselines (Before Fine-tuning)

| MÃ´ hÃ¬nh | Sai sá»‘ (MAE) | Ghi chÃº |
|----------|---------------|---------|
| Random Model | $423.71 | Ráº¥t kÃ©m |
| Linear Regression | $52.94 | Cáº£i thiá»‡n rÃµ |
| Word2Vec | $50.94 | á»”n Ä‘á»‹nh |
| HistGradientBoosting | **$50.74** | Tá»‘t nháº¥t trÆ°á»›c fine-tune |

---

## ğŸ§© 4. Fine-tuning Techniques

- **LoRA (Low-Rank Adaptation):** Giáº£m sá»‘ tham sá»‘ cáº§n huáº¥n luyá»‡n, tÄƒng tá»‘c Ä‘á»™ fine-tuning.  
  ğŸ”— [LoRA Guide - Hugging Face](https://huggingface.co/docs/peft/conceptual_guides/lora)

- **QLoRA (Quantized LoRA):** Tá»‘i Æ°u bá»™ nhá»› cho mÃ´ hÃ¬nh lá»›n.  
  ğŸ“„ [QLoRA Paper (2023)](https://arxiv.org/abs/2305.14314)

---

## âš™ï¸ 5. Key Hyperparameters

### ğŸ”¹ R (Rank)
- XÃ¡c Ä‘á»‹nh kÃ­ch thÆ°á»›c khÃ´ng gian con trong LoRA.  
- Nhá» hÆ¡n â†’ nhanh hÆ¡n, Ã­t tham sá»‘ hÆ¡n, nhÆ°ng Ä‘á»™ chÃ­nh xÃ¡c cÃ³ thá»ƒ giáº£m.  
ğŸ“˜ [Docs: PEFT LoRA Parameters](https://huggingface.co/docs/peft/main/en/package_reference/lora)

### ğŸ”¹ Alpha
- Há»‡ sá»‘ khuáº¿ch Ä‘áº¡i áº£nh hÆ°á»Ÿng cá»§a tham sá»‘ LoRA lÃªn trá»ng sá»‘ gá»‘c.  
ğŸ“˜ [Docs: LoRA Alpha Scaling](https://arxiv.org/pdf/2106.09685.pdf)

### ğŸ”¹ Target Modules
- CÃ¡c lá»›p trong mÃ´ hÃ¬nh mÃ  LoRA Ä‘Æ°á»£c Ã¡p dá»¥ng: `"q_proj"`, `"k_proj"`, `"v_proj"`, `"o_proj"`.  
ğŸ“˜ [Docs: Choosing Target Modules](https://huggingface.co/docs/peft/conceptual_guides/lora)

---

## ğŸ§® 6. LLMs Training Workflow

1. **Forward Pass:** Dá»¯ liá»‡u Ä‘i qua mÃ´ hÃ¬nh â†’ dá»± Ä‘oÃ¡n token tiáº¿p theo.  
2. **Loss Calculation:** So sÃ¡nh Ä‘áº§u ra vá»›i nhÃ£n tháº­t Ä‘á»ƒ tÃ­nh loss.  
3. **Backward Pass:** TÃ­nh gradient theo cÃ¡c tham sá»‘.  
4. **Optimization:**  


ğŸ“˜ [Deep Learning Book â€“ Optimization](https://www.deeplearningbook.org/)

---

## âš™ï¸ 7. Training Configuration

### ğŸ”¸ SFTTrainer
- **Epochs:** 3â€“4  
- **Batch size:** 1 Ã— 16 (gradient accumulation)  
- **Optimizer:** `paged_adamw_32bit` â€“ tá»‘i Æ°u bá»™ nhá»› khi fine-tune mÃ´ hÃ¬nh lá»›n  
ğŸ“˜ [PagedAdamW Optimizer](https://huggingface.co/docs/transformers/main/en/main_classes/optimizer_schedules#paged-adamw)

### ğŸ”¸ Learning Rate
- QuÃ¡ cao â†’ nháº£y khá»i Ä‘iá»ƒm tá»‘i Æ°u  
- QuÃ¡ tháº¥p â†’ há»c cháº­m hoáº·c káº¹t local minimum  

### ğŸ”¸ Scheduler
- **Cosine Scheduler:** giáº£m learning rate theo sÃ³ng cosine  
ğŸ“˜ [Cosine LR Scheduler](https://arxiv.org/abs/1608.03983)

### ğŸ”¸ Warmup Ratio
- Giai Ä‘oáº¡n khá»Ÿi Ä‘á»™ng: learning rate tÄƒng dáº§n trÆ°á»›c khi giáº£m theo scheduler.

---

## ğŸ§° 8. Useful Tools & Frameworks for LLMs

| CÃ´ng cá»¥ | MÃ´ táº£ |
|----------|--------|
| ğŸ¤— **Hugging Face** | MÃ´ hÃ¬nh, datasets, leaderboard, demo app |
| ğŸ”— **LangChain** | Káº¿t ná»‘i nhiá»u thao tÃ¡c vá»›i LLM qua API Ä‘Æ¡n giáº£n |
| ğŸ›ï¸ **Gradio** | Táº¡o giao diá»‡n demo UI nhanh gá»n |
| ğŸ“Š **Weights & Biases** | PhÃ¢n tÃ­ch & trá»±c quan hÃ³a quÃ¡ trÃ¬nh huáº¥n luyá»‡n |
| ğŸ§© **Streamlit / Dash / Mesop** | Alternatives Ä‘á»ƒ xÃ¢y dá»±ng giao diá»‡n tÆ°Æ¡ng tÃ¡c |
| â˜ï¸ **Google Colab** | Notebook Cloud miá»…n phÃ­ |
| âš¡ **Modal.com** | Ná»n táº£ng serverless triá»ƒn khai mÃ´ hÃ¬nh AI |

---

## ğŸ“ 9. Há»c liá»‡u & TÃ i nguyÃªn
> â€œNáº¿u báº¡n tháº­t sá»± muá»‘n Ä‘Ã o sÃ¢u vÃ o lÄ©nh vá»±c LLMs vÃ  Fine-tuning, hÃ£y báº¯t Ä‘áº§u tá»« nhá»¯ng khÃ³a há»c uy tÃ­n.â€

ğŸ“š **NVIDIA Learning Catalog:**  
ğŸ”— [https://nvdam.widen.net/s/wlbgbqr7cj/nvidia-learning-training-course-catalog](https://nvdam.widen.net/s/wlbgbqr7cj/nvidia-learning-training-course-catalog)

ğŸ¥ **Llm Course (Video reference)**  

---

## ğŸ§¾ 10. Notes
> Táº¥t cáº£ kiáº¿n thá»©c vÃ  ná»™i dung trong dá»± Ã¡n nÃ y Ä‘Æ°á»£c tá»•ng há»£p tá»« quÃ¡ trÃ¬nh há»c táº­p vÃ  nghiÃªn cá»©u cÃ¡ nhÃ¢n.  
> **KhÃ´ng cÃ³ má»¥c Ä‘Ã­ch thÆ°Æ¡ng máº¡i â€“ purely educational use.**

---

### âœï¸ Author: **Tráº§n Tháº¿ Anh**  
ğŸš€ Developer | AI Engineer (NLP & Computer Vision)  
ğŸ“§ *Contact:* updating...
