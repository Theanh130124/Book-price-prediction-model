{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GJsVFjZngvTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "896fdcf5-4fe7-4ed3-9067-42f4b2411b0f"
      },
      "id": "GJsVFjZngvTL",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip installs\n",
        "\n",
        "!pip install -q datasets peft requests torch bitsandbytes transformers trl accelerate sentencepiece matplotlib"
      ],
      "metadata": {
        "id": "FLRAri-2g-rG"
      },
      "id": "FLRAri-2g-rG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, set_seed, BitsAndBytesConfig\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import wandb\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "azvSvFSthALu"
      },
      "id": "azvSvFSthALu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set check point tracker\n",
        "# Path to store the checkpoint tracking information\n",
        "def create_checkpoint_tracker():\n",
        "    checkpoint_file = \"checkpoint_tracker.py\"\n",
        "\n",
        "    with open(checkpoint_file, \"w\") as f:\n",
        "        f.write(\"\"\"\n",
        "def get_latest_step():\n",
        "    try:\n",
        "        with open(\"latest_step.txt\", \"r\") as f:\n",
        "            return int(f.read().strip())\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def save_latest_step(step):\n",
        "    with open(\"latest_step.txt\", \"w\") as f:\n",
        "        f.write(str(step))\n",
        "\"\"\")\n",
        "\n",
        "create_checkpoint_tracker()\n",
        "from checkpoint_tracker import get_latest_step, save_latest_step"
      ],
      "metadata": {
        "id": "bOJ6jQrHhYE6"
      },
      "id": "bOJ6jQrHhYE6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup STF-Config"
      ],
      "metadata": {
        "id": "75h6xOen5UFw"
      },
      "id": "75h6xOen5UFw"
    },
    {
      "cell_type": "code",
      "source": [
        "# resume training function\n",
        "def train_or_resume(\n",
        "    base_model_name,  #Model gốc để fine-tune (vd: \"meta-llama/Llama-3-8B\")\n",
        "    hf_model_name, #Tên repo Hugging Face để push kết quả lên\n",
        "    train_dataset,\n",
        "    lora_config, #Cấu hình LoRA (Low-Rank Adaptation)\n",
        "    steps_per_session=500, #Số bước huấn luyện mỗi phiên (session)\n",
        "    max_total_steps=1000, #Tổng số bước huấn luyện tối đa\n",
        "    batch_size=1,\n",
        "    grad_accum_steps=16, #Gradient accumulation steps (tích lũy gradient để tiết kiệm VRAM)\n",
        "    save_steps=100 #Lưu checkpoint mỗi bao nhiêu bước\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a model or resume training from the latest checkpoint on Hugging Face.\n",
        "\n",
        "    Args:\n",
        "        base_model_name: Original model to fine-tune or 'resume' to continue training\n",
        "        hf_model_name: HF repo name to save model to (username/model-name)\n",
        "        train_dataset: Dataset to train on\n",
        "        lora_config: LoRA configuration\n",
        "        steps_per_session: How many steps to train in this session\n",
        "        max_total_steps: Maximum number of steps to train overall\n",
        "        batch_size: Batch size for training\n",
        "        grad_accum_steps: Gradient accumulation steps\n",
        "        save_steps: How often to save checkpoints\n",
        "    \"\"\"\n",
        "    # Get the latest step we've trained to\n",
        "    latest_step = get_latest_step()\n",
        "\n",
        "    # Check if we've already reached the max steps\n",
        "    if latest_step >= max_total_steps:\n",
        "        print(f\"Training already completed! Reached {latest_step}/{max_total_steps} steps\")\n",
        "        return\n",
        "\n",
        "    # Calculate how many steps to train in this session\n",
        "    steps_this_session = min(steps_per_session, max_total_steps - latest_step)\n",
        "    print(f\"Training for {steps_this_session} steps (total progress: {latest_step}/{max_total_steps})\")\n",
        "\n",
        "    # Set up tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    # Configure quantization -> Qlora 4bit\n",
        "    quant_config = transformers.BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "\n",
        "    # Check if we need to resume training\n",
        "    try:\n",
        "        if latest_step > 0:\n",
        "            print(f\"Resuming from checkpoint at step {latest_step}\")\n",
        "            # Load from Hugging Face\n",
        "            base_model = AutoModelForCausalLM.from_pretrained(\n",
        "                hf_model_name,\n",
        "                quantization_config=quant_config,\n",
        "                device_map=\"auto\",\n",
        "            )\n",
        "        else:\n",
        "            print(\"Starting training from base model\")\n",
        "            # Start fresh\n",
        "            base_model = AutoModelForCausalLM.from_pretrained(\n",
        "                base_model_name,\n",
        "                quantization_config=quant_config,\n",
        "                device_map=\"auto\",\n",
        "            )\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model, starting fresh: {e}\")\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            base_model_name,\n",
        "            quantization_config=quant_config,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "\n",
        "    # Configure training parameters #SFTConfig #Supervised Fine-Tuning\n",
        "    train_params = SFTConfig(\n",
        "        output_dir=f\"./checkpoints\",  #Thư mục nơi model và checkpoint sẽ được lưu tạm thời trong quá trình huấn luyện.\n",
        "        num_train_epochs=1, #Có thể tăng 4\n",
        "        max_steps=steps_this_session, #Giới hạn số bước train tối đa trong phiên huấn luyện hiện tại.\n",
        "        per_device_train_batch_size=batch_size, #Batch size thực tế (số mẫu dữ liệu đưa vào GPU mỗi lần huấn luyện).\n",
        "        gradient_accumulation_steps=grad_accum_steps, #16 -> Số bước tích lũy gradient trước khi cập nhật trọng số.\n",
        "        optim=\"paged_adamw_32bit\",#Ở đây dùng paged_adamw_32bit – phiên bản tối ưu bộ nhớ (paged) của AdamW, thích hợp khi train mô hình lớn (LLM) với LoRA và 4bit quantization.\n",
        "        save_steps=save_steps, #sau 100 steps ở trên\n",
        "        logging_steps=20, #Ghi log (hiển thị loss, learning rate, …) mỗi 20 bước.\n",
        "        learning_rate=1e-4,  #Tốc độ học (learning rate) = 0.0001. -> càng cao càng học dễ sai , càng nhỏ càng học chậm (1e-4 hoặc 5e-5)\n",
        "        weight_decay=0.001, #Hệ số “giảm trọng số” — tránh overfitting bằng cách phạt các trọng số lớn.\n",
        "        fp16=False, #Kiểu dữ liệu huấn luyện (precision).\n",
        "        bf16=True,#Kiểu dữ liệu huấn luyện (precision).\n",
        "        max_grad_norm=0.3, #Giới hạn độ lớn của gradient (gradient clipping).\n",
        "        warmup_ratio=0.03, #Trong 3% đầu của quá trình train, learning rate sẽ tăng dần từ 0 → 1e-4.\n",
        "        group_by_length=True, #Gộp các mẫu có độ dài gần nhau trong batch.\n",
        "        lr_scheduler_type=\"cosine\",#Kiểu giảm learning rate theo đường cong cosine. #giảm dần learning rate -> bắt đầu giảm nhẹ giảm mạnh về sau và căn bằng về cuối\n",
        "        push_to_hub=True, #Cho phép tự động đẩy model lên Hugging Face Hub sau khi train xong.\n",
        "        hub_model_id=hf_model_name, #Tên repository trên Hugging Face (ví dụ \"theanhtran/bookbot-lora-v1\")\n",
        "        hub_private_repo=True #Repo được để private (chỉ bạn xem được).\n",
        "    )\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = SFTTrainer(\n",
        "        model=base_model,\n",
        "        train_dataset=train_dataset,\n",
        "        peft_config=lora_config,\n",
        "        args=train_params,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Push to Hugging Face Hub\n",
        "    trainer.model.push_to_hub(hf_model_name, private=True)\n",
        "\n",
        "    # Update and save the latest step count\n",
        "    save_latest_step(latest_step + steps_this_session)\n",
        "\n",
        "    print(f\"Completed training session ({latest_step + steps_this_session}/{max_total_steps} steps)\")\n",
        "    print(f\"Model saved to HuggingFace: {hf_model_name}\")\n",
        "\n",
        "    return latest_step + steps_this_session"
      ],
      "metadata": {
        "id": "3oZZhayPhZua"
      },
      "id": "3oZZhayPhZua",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine-tune and save model huggingface and log with wandb"
      ],
      "metadata": {
        "id": "35WDbjGv5MSZ"
      },
      "id": "35WDbjGv5MSZ"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Login to Hugging Face\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "LOG_TO_WANDB = True\n",
        "# Log in to Weights & Biases\n",
        "wandb_api_key = userdata.get('WANDB_API_KEY')  #https://wandb.ai/site/  -> để xem chi tiết quá trình huấn luyện\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
        "wandb.login()\n",
        "\n",
        "# Configure Weights & Biases to record against our project\n",
        "os.environ[\"WANDB_PROJECT\"] = \"llms_fine_tune\"\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" if LOG_TO_WANDB else \"end\"\n",
        "os.environ[\"WANDB_WATCH\"] = \"gradients\"\n",
        "\n",
        "# Model and repository names\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "HF_USER = \"tta1301\"\n",
        "PROJECT_NAME = \"pricer\"\n",
        "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_NAME}\"\n",
        "\n",
        "# Load your dataset\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(f\"{HF_USER}/bookpricer-data-clone\")\n",
        "train_data = dataset['train'].shuffle(seed=123).select(range(min(50000, len(dataset['train']))))\n",
        "\n",
        "\n",
        "#LoRa ở đây\n",
        "# LoRA configuration\n",
        "lora_parameters = LoraConfig(\n",
        "    lora_alpha=16, # = 2.r\n",
        "    lora_dropout=0.1,  #Dropout\n",
        "    r=8,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        ")\n",
        "\n",
        "# Create the response template and data collator\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "# Train or resume\n",
        "current_step = train_or_resume(\n",
        "    base_model_name=BASE_MODEL,\n",
        "    hf_model_name=HUB_MODEL_NAME,\n",
        "    train_dataset=train_data,\n",
        "    lora_config=lora_parameters,\n",
        "    steps_per_session=300,  # Train for 300 steps at a time (adjust as needed)\n",
        "    max_total_steps=1000,   # Maximum total steps\n",
        "    batch_size=1,\n",
        "    grad_accum_steps=16,\n",
        "    save_steps=100          # Save every 100 steps\n",
        ")\n",
        "\n",
        "print(f\"Current training progress: {current_step}/1000 steps\")"
      ],
      "metadata": {
        "id": "HjBfdpGEhbzq"
      },
      "id": "HjBfdpGEhbzq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TEst our fine-tuned model"
      ],
      "metadata": {
        "id": "aQ8JtURuhd_r"
      },
      "id": "aQ8JtURuhd_r"
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "PROJECT_NAME = \"pricer\"\n",
        "HF_USER = \"Timi1511\" # your HF name here! Or use mine if you just want to reproduce my results.\n",
        "\n",
        "# The run itself\n",
        "\n",
        "RUN_NAME = \"2025-04-16_12.53.07\"\n",
        "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
        "REVISION = \"71dc08b7f8c91e521ed510a5752f97187e415a2e\" # or REVISION = None\n",
        "FINETUNED_MODEL = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
        "\n",
        "# Data\n",
        "DATASET_NAME = f\"{HF_USER}/bookpricer-data\"\n",
        "\n",
        "# Hyperparameters for QLoRA\n",
        "\n",
        "QUANT_4_BIT = True\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Used for writing to output in color\n",
        "\n",
        "GREEN = \"\\033[92m\"\n",
        "YELLOW = \"\\033[93m\"\n",
        "RED = \"\\033[91m\"\n",
        "RESET = \"\\033[0m\"\n",
        "COLOR_MAP = {\"red\":RED, \"orange\": YELLOW, \"green\": GREEN}"
      ],
      "metadata": {
        "id": "TyfM9ZH0hf67"
      },
      "id": "TyfM9ZH0hf67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Hugging face\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "c4oixzA9hibz"
      },
      "id": "c4oixzA9hibz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(DATASET_NAME)\n",
        "train = dataset['train']\n",
        "test = dataset['test']"
      ],
      "metadata": {
        "id": "klnCUNXNhkWF"
      },
      "id": "klnCUNXNhkWF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test[0]"
      ],
      "metadata": {
        "id": "Vv3-SCBwhlo1"
      },
      "id": "Vv3-SCBwhlo1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick the right quantization\n",
        "\n",
        "if QUANT_4_BIT:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        "  )\n",
        "else:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.bfloat16\n",
        "  )"
      ],
      "metadata": {
        "id": "YkqUsqC5hnZd"
      },
      "id": "YkqUsqC5hnZd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Tokenizer and the Model\n",
        "from peft import PeftModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Load the fine-tuned model with PEFT\n",
        "if REVISION:\n",
        "  fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL, revision=REVISION)\n",
        "else:\n",
        "  fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL)\n",
        "\n",
        "\n",
        "print(f\"Memory footprint: {fine_tuned_model.get_memory_footprint() / 1e6:.1f} MB\")"
      ],
      "metadata": {
        "id": "wiQ0Tnychop1"
      },
      "id": "wiQ0Tnychop1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model"
      ],
      "metadata": {
        "id": "YFAVcZOJhpvN"
      },
      "id": "YFAVcZOJhpvN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_price(s):\n",
        "    if \"Price is $\" in s:\n",
        "      contents = s.split(\"Price is $\")[1]\n",
        "      contents = contents.replace(',','')\n",
        "      match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", contents)\n",
        "      return float(match.group()) if match else 0\n",
        "    return 0"
      ],
      "metadata": {
        "id": "Nti7wL_UhrZV"
      },
      "id": "Nti7wL_UhrZV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_price(\"Price is $abc fand $999\")"
      ],
      "metadata": {
        "id": "4WMDepmyhsiF"
      },
      "id": "4WMDepmyhsiF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Original prediction function takes the most likely next token\n",
        "\n",
        "def model_predict(prompt):\n",
        "    set_seed(123)\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    attention_mask = torch.ones(inputs.shape, device=\"cuda\")\n",
        "    outputs = fine_tuned_model.generate(inputs, attention_mask=attention_mask, max_new_tokens=3, num_return_sequences=1)\n",
        "    response = tokenizer.decode(outputs[0])\n",
        "    return extract_price(response)"
      ],
      "metadata": {
        "id": "fJJnRaCqhtTF"
      },
      "id": "fJJnRaCqhtTF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# An improved prediction function takes a weighted average of the top 3 choices\n",
        "# This code would be more complex if we couldn't take advantage of the fact\n",
        "# That Llama generates 1 token for any 3 digit number\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "top_K = 3\n",
        "\n",
        "def improved_model_predict(prompt, device=\"cuda\"):\n",
        "    set_seed(123)\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    attention_mask = torch.ones(inputs.shape, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = fine_tuned_model(inputs, attention_mask=attention_mask)\n",
        "        next_token_logits = outputs.logits[:, -1, :].to('cpu')\n",
        "\n",
        "    next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
        "    top_prob, top_token_id = next_token_probs.topk(top_K)\n",
        "    prices, weights = [], []\n",
        "    for i in range(top_K):\n",
        "      predicted_token = tokenizer.decode(top_token_id[0][i])\n",
        "      probability = top_prob[0][i]\n",
        "      try:\n",
        "        result = float(predicted_token)\n",
        "      except ValueError as e:\n",
        "        result = 0.0\n",
        "      if result > 0:\n",
        "        prices.append(result)\n",
        "        weights.append(probability)\n",
        "    if not prices:\n",
        "      return 0.0, 0.0\n",
        "    total = sum(weights)\n",
        "    weighted_prices = [price * weight / total for price, weight in zip(prices, weights)]\n",
        "    return sum(weighted_prices).item()"
      ],
      "metadata": {
        "id": "7V4ktTJRhuO-"
      },
      "id": "7V4ktTJRhuO-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tester:\n",
        "\n",
        "    def __init__(self, predictor, data, title=None, size=300):\n",
        "        self.predictor = predictor\n",
        "        self.data = data\n",
        "        self.title = title or predictor.__name__.replace(\"_\", \" \").title()\n",
        "        self.size = size\n",
        "        self.guesses = []\n",
        "        self.truths = []\n",
        "        self.errors = []\n",
        "        self.sles = []\n",
        "        self.colors = []\n",
        "\n",
        "    def color_for(self, error, truth):\n",
        "        if error<40 or error/truth < 0.2:\n",
        "            return \"green\"\n",
        "        elif error<80 or error/truth < 0.4:\n",
        "            return \"orange\"\n",
        "        else:\n",
        "            return \"red\"\n",
        "\n",
        "    def run_datapoint(self, i):\n",
        "        datapoint = self.data[i]\n",
        "        guess = self.predictor(datapoint[\"text\"])\n",
        "        truth = datapoint[\"price\"]\n",
        "        error = abs(guess - truth)\n",
        "        log_error = math.log(truth+1) - math.log(guess+1)\n",
        "        sle = log_error ** 2\n",
        "        color = self.color_for(error, truth)\n",
        "        title = datapoint[\"text\"].split(\"\\n\\n\")[1][:20] + \"...\"\n",
        "        self.guesses.append(guess)\n",
        "        self.truths.append(truth)\n",
        "        self.errors.append(error)\n",
        "        self.sles.append(sle)\n",
        "        self.colors.append(color)\n",
        "        print(f\"{COLOR_MAP[color]}{i+1}: Guess: ${guess:,.2f} Truth: ${truth:,.2f} Error: ${error:,.2f} SLE: {sle:,.2f} Item: {title}{RESET}\")\n",
        "\n",
        "    def chart(self, title):\n",
        "        max_error = max(self.errors)\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        max_val = max(max(self.truths), max(self.guesses))\n",
        "        plt.plot([0, max_val], [0, max_val], color='deepskyblue', lw=2, alpha=0.6)\n",
        "        plt.scatter(self.truths, self.guesses, s=3, c=self.colors)\n",
        "        plt.xlabel('Ground Truth')\n",
        "        plt.ylabel('Model Estimate')\n",
        "        plt.xlim(0, max_val)\n",
        "        plt.ylim(0, max_val)\n",
        "        plt.title(title)\n",
        "        plt.show()\n",
        "\n",
        "    def report(self):\n",
        "        average_error = sum(self.errors) / self.size\n",
        "        rmsle = math.sqrt(sum(self.sles) / self.size)\n",
        "        hits = sum(1 for color in self.colors if color==\"green\")\n",
        "        title = f\"{self.title} Error=${average_error:,.2f} RMSLE={rmsle:,.2f} Hits={hits/self.size*100:.1f}%\"\n",
        "        self.chart(title)\n",
        "\n",
        "    def run(self):\n",
        "        self.error = 0\n",
        "        for i in range(self.size):\n",
        "            self.run_datapoint(i)\n",
        "        self.report()\n",
        "\n",
        "    @classmethod\n",
        "    def test(cls, function, data):\n",
        "        cls(function, data).run()"
      ],
      "metadata": {
        "id": "26V3NVGvhvdu"
      },
      "id": "26V3NVGvhvdu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tester.test(improved_model_predict, test)"
      ],
      "metadata": {
        "id": "dtTcHOQBhwiG"
      },
      "id": "dtTcHOQBhwiG",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}